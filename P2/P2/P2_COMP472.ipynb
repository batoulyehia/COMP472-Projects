{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before smoothing:\n",
      "                  word     neg     pos\n",
      "0                   i  1014.0   985.0\n",
      "1              bought    43.0    33.0\n",
      "2                this   605.0   621.0\n",
      "3               album    57.0    55.0\n",
      "4             because    72.0    58.0\n",
      "5               loved    11.0     6.0\n",
      "6                 the  2109.0  2178.0\n",
      "7               title     6.0     3.0\n",
      "8                song    25.0    18.0\n",
      "9                   .  1896.0  1920.0\n",
      "10                 it   742.0   745.0\n",
      "11                 's   297.0   312.0\n",
      "12               such    29.0    19.0\n",
      "13                  a   963.0  1025.0\n",
      "14              great    63.0   115.0\n",
      "15                  ,  1741.0  1665.0\n",
      "16                how    76.0    58.0\n",
      "17                bad    43.0    16.0\n",
      "18                can    90.0   126.0\n",
      "19               rest    14.0     7.0\n",
      "20                 of   857.0   898.0\n",
      "21                 be   170.0   170.0\n",
      "22              right    22.0    28.0\n",
      "23                  ?   104.0    52.0\n",
      "24               well    43.0    69.0\n",
      "25              songs    43.0    27.0\n",
      "26                are   217.0   263.0\n",
      "27               just   149.0   101.0\n",
      "28             filler     9.0     0.0\n",
      "29                and  1029.0  1207.0\n",
      "...               ...     ...     ...\n",
      "10653         mccoury     0.0     1.0\n",
      "10654           altan     0.0     1.0\n",
      "10655         frankie     0.0     1.0\n",
      "10656           gavin     0.0     1.0\n",
      "10657           jerry     0.0     1.0\n",
      "10658         douglas     0.0     1.0\n",
      "10659            earl     0.0     1.0\n",
      "10660         scruggs     0.0     1.0\n",
      "10661          seamus     0.0     1.0\n",
      "10662            egan     0.0     1.0\n",
      "10663       requiring     0.0     1.0\n",
      "10664        gimmicks     0.0     1.0\n",
      "10665     mountaineer     0.0     1.0\n",
      "10666       free&quot     0.0     1.0\n",
      "10667   children&quot     0.0     1.0\n",
      "10668  wandering&quot     0.0     1.0\n",
      "10669      riley&quot     0.0     1.0\n",
      "10670          labour     0.0     1.0\n",
      "10671        atlantic     0.0     1.0\n",
      "10672       autofocus     0.0     1.0\n",
      "10673           macro     0.0     1.0\n",
      "10674             105     0.0     1.0\n",
      "10675           24-50     0.0     1.0\n",
      "10676            mate     0.0     1.0\n",
      "10677      categories     0.0     1.0\n",
      "10678       excerises     0.0     1.0\n",
      "10679      engelbreit     0.0     1.0\n",
      "10680          crafts     0.0     1.0\n",
      "10681       gardening     0.0     1.0\n",
      "10682          sewing     0.0     1.0\n",
      "\n",
      "[10683 rows x 3 columns]\n",
      "After smoothing:\n",
      "                  word     neg     pos\n",
      "0                   i  1014.5   985.5\n",
      "1              bought    43.5    33.5\n",
      "2                this   605.5   621.5\n",
      "3               album    57.5    55.5\n",
      "4             because    72.5    58.5\n",
      "5               loved    11.5     6.5\n",
      "6                 the  2109.5  2178.5\n",
      "7               title     6.5     3.5\n",
      "8                song    25.5    18.5\n",
      "9                   .  1896.5  1920.5\n",
      "10                 it   742.5   745.5\n",
      "11                 's   297.5   312.5\n",
      "12               such    29.5    19.5\n",
      "13                  a   963.5  1025.5\n",
      "14              great    63.5   115.5\n",
      "15                  ,  1741.5  1665.5\n",
      "16                how    76.5    58.5\n",
      "17                bad    43.5    16.5\n",
      "18                can    90.5   126.5\n",
      "19               rest    14.5     7.5\n",
      "20                 of   857.5   898.5\n",
      "21                 be   170.5   170.5\n",
      "22              right    22.5    28.5\n",
      "23                  ?   104.5    52.5\n",
      "24               well    43.5    69.5\n",
      "25              songs    43.5    27.5\n",
      "26                are   217.5   263.5\n",
      "27               just   149.5   101.5\n",
      "28             filler     9.5     0.5\n",
      "29                and  1029.5  1207.5\n",
      "...               ...     ...     ...\n",
      "10653         mccoury     0.5     1.5\n",
      "10654           altan     0.5     1.5\n",
      "10655         frankie     0.5     1.5\n",
      "10656           gavin     0.5     1.5\n",
      "10657           jerry     0.5     1.5\n",
      "10658         douglas     0.5     1.5\n",
      "10659            earl     0.5     1.5\n",
      "10660         scruggs     0.5     1.5\n",
      "10661          seamus     0.5     1.5\n",
      "10662            egan     0.5     1.5\n",
      "10663       requiring     0.5     1.5\n",
      "10664        gimmicks     0.5     1.5\n",
      "10665     mountaineer     0.5     1.5\n",
      "10666       free&quot     0.5     1.5\n",
      "10667   children&quot     0.5     1.5\n",
      "10668  wandering&quot     0.5     1.5\n",
      "10669      riley&quot     0.5     1.5\n",
      "10670          labour     0.5     1.5\n",
      "10671        atlantic     0.5     1.5\n",
      "10672       autofocus     0.5     1.5\n",
      "10673           macro     0.5     1.5\n",
      "10674             105     0.5     1.5\n",
      "10675           24-50     0.5     1.5\n",
      "10676            mate     0.5     1.5\n",
      "10677      categories     0.5     1.5\n",
      "10678       excerises     0.5     1.5\n",
      "10679      engelbreit     0.5     1.5\n",
      "10680          crafts     0.5     1.5\n",
      "10681       gardening     0.5     1.5\n",
      "10682          sewing     0.5     1.5\n",
      "\n",
      "[10683 rows x 3 columns]\n",
      "51469.5\n",
      "                 word        neg        pos\n",
      "0                   i  -3.929271  -3.955596\n",
      "1              bought  -7.078661  -7.337199\n",
      "2                this  -4.445368  -4.416609\n",
      "3               album  -6.799637  -6.832362\n",
      "4             because  -6.567836  -6.779718\n",
      "5               loved  -8.409075  -8.976943\n",
      "6                 the  -3.197216  -3.162353\n",
      "7               title  -8.979620  -9.595982\n",
      "8                song  -7.612744  -7.930974\n",
      "9                   .  -3.303657  -3.288404\n",
      "10                 it  -4.241399  -4.234690\n",
      "11                 's  -5.156008  -5.104140\n",
      "12               such  -7.467032  -7.878330\n",
      "13                  a  -3.980850  -3.915809\n",
      "14              great  -6.700382  -6.099474\n",
      "15                  ,  -3.388920  -3.430864\n",
      "16                how  -6.514132  -6.779718\n",
      "17                bad  -7.078661  -8.045384\n",
      "18                can  -6.346072  -6.008502\n",
      "19               rest  -8.177274  -8.833842\n",
      "20                 of  -4.097401  -4.048018\n",
      "21                 be  -5.712687  -5.710009\n",
      "22              right  -7.737907  -7.498841\n",
      "23                  ?  -6.202235  -6.887932\n",
      "24               well  -7.078661  -6.607418\n",
      "25              songs  -7.078661  -7.534559\n",
      "26                are  -5.469223  -5.274691\n",
      "27               just  -5.844126  -6.228686\n",
      "28             filler  -8.600130 -11.541892\n",
      "29                and  -3.914594  -3.752437\n",
      "...               ...        ...        ...\n",
      "10653         mccoury -11.544569 -10.443280\n",
      "10654           altan -11.544569 -10.443280\n",
      "10655         frankie -11.544569 -10.443280\n",
      "10656           gavin -11.544569 -10.443280\n",
      "10657           jerry -11.544569 -10.443280\n",
      "10658         douglas -11.544569 -10.443280\n",
      "10659            earl -11.544569 -10.443280\n",
      "10660         scruggs -11.544569 -10.443280\n",
      "10661          seamus -11.544569 -10.443280\n",
      "10662            egan -11.544569 -10.443280\n",
      "10663       requiring -11.544569 -10.443280\n",
      "10664        gimmicks -11.544569 -10.443280\n",
      "10665     mountaineer -11.544569 -10.443280\n",
      "10666       free&quot -11.544569 -10.443280\n",
      "10667   children&quot -11.544569 -10.443280\n",
      "10668  wandering&quot -11.544569 -10.443280\n",
      "10669      riley&quot -11.544569 -10.443280\n",
      "10670          labour -11.544569 -10.443280\n",
      "10671        atlantic -11.544569 -10.443280\n",
      "10672       autofocus -11.544569 -10.443280\n",
      "10673           macro -11.544569 -10.443280\n",
      "10674             105 -11.544569 -10.443280\n",
      "10675           24-50 -11.544569 -10.443280\n",
      "10676            mate -11.544569 -10.443280\n",
      "10677      categories -11.544569 -10.443280\n",
      "10678       excerises -11.544569 -10.443280\n",
      "10679      engelbreit -11.544569 -10.443280\n",
      "10680          crafts -11.544569 -10.443280\n",
      "10681       gardening -11.544569 -10.443280\n",
      "10682          sewing -11.544569 -10.443280\n",
      "\n",
      "[10683 rows x 3 columns]\n",
      "Total labels:\n",
      " 595\n",
      "negtive labels:\n",
      " 283\n",
      "postive labels:\n",
      " 312\n",
      "negtive probablity after smoothing and log:\n",
      " -0.7431145079023922\n",
      "postive probablity after smoothing and log:\n",
      " -0.6455582177361474\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "from codecs import open\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read document\n",
    "# Ref: Project 2 instruction\n",
    "def read_documents(doc_file):\n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            #print(words)\n",
    "            docs.append(words[3:])\n",
    "            labels.append(words[1])\n",
    "    return docs, labels\n",
    "\n",
    "# Seperate list it by class\n",
    "def separate_by_class(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated\n",
    "\n",
    "# Get the sum of pos column\n",
    "def sum_total_pos(dataframe):\n",
    "    return dataframe['pos'].sum()\n",
    "\n",
    "# Get the sum of neg column\n",
    "def sum_total_neg(dataframe):\n",
    "    return dataframe['neg'].sum()\n",
    "\n",
    "# Smoothing dataframe\n",
    "def smooth_data_frame(dataframe):\n",
    "    dataframe['pos'] = dataframe['pos'].add(0.5)\n",
    "    dataframe['neg'] = dataframe['neg'].add(0.5)\n",
    "    return dataframe\n",
    "\n",
    "# Calculate logarithmic probability\n",
    "def calculate_probability(value,total):\n",
    "    return np.log(value/total)\n",
    "\n",
    "# Calculate logarithmic probabilities for dataframe\n",
    "def calculate_probabilities(dataframe):\n",
    "    sum_pos = sum_total_pos(dataframe)\n",
    "    print(sum_pos)\n",
    "    sum_neg = sum_total_neg(dataframe)\n",
    "    dataframe['pos'] =  np.log(dataframe['pos'] / sum_pos)\n",
    "    dataframe['neg'] =  np.log(dataframe['neg'] / sum_neg)\n",
    "    return dataframe\n",
    "\n",
    "# Add label to docs\n",
    "def add_label_to_docs(docs,labels):\n",
    "    for index in range(len(docs)):\n",
    "        docs[index].append(labels[index])\n",
    "    return docs\n",
    "\n",
    "def train_nb(documents, labels):\n",
    "    #return the data you need to classify new instances\n",
    "    new_docs = add_label_to_docs(documents,labels)\n",
    "    # Separate docs by label\n",
    "    train_docs = np.array(documents)\n",
    "    seperate = separate_by_class(documents)\n",
    "    train_docs_neg_freqs = Counter(w for doc in seperate['neg'] for w in doc)\n",
    "    train_docs_pos_freqs = Counter(w for doc in seperate['pos'] for w in doc)\n",
    "    df_neg = pd.DataFrame.from_dict(train_docs_neg_freqs, orient='index').reset_index()\n",
    "    df_neg = df_neg.rename(columns={'index':'word', 0:'neg'})\n",
    "    df_pos = pd.DataFrame.from_dict(train_docs_pos_freqs, orient='index').reset_index()\n",
    "    df_pos = df_pos.rename(columns={'index':'word', 0:'pos'})\n",
    "    \n",
    "    # Union Join two dataframe\n",
    "    # Ref: https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/\n",
    "    df_merge = pd.merge(df_neg,df_pos,on='word',how='outer')\n",
    "    df_merge = df_merge.fillna(0)\n",
    "    print('Before smoothing:\\n',df_merge)\n",
    "    df_merge = smooth_data_frame(df_merge)\n",
    "    print('After smoothing:\\n',df_merge)\n",
    "    \n",
    "    #return a dataframe with all probabilities\n",
    "    return calculate_probabilities(df_merge)\n",
    "\n",
    "\n",
    "# def score_doc_label(document, label, <SOMETHING>):\n",
    "# ...\n",
    "# (return the log probability)\n",
    "\n",
    "\n",
    "\n",
    "# def classify_nb(document, <SOMETHING>):\n",
    "# ...\n",
    "# (return the guess of the classifier)\n",
    "\n",
    "\n",
    "# def classify_documents(docs, <SOMETHING>):\n",
    "# ...\n",
    "# (return the classifier's predictions for all documents in the collection)\n",
    " \n",
    "\n",
    "# def accuracy(true_labels, guessed_labels):\n",
    "# ...\n",
    "# (return the accuracy)\n",
    "\n",
    "\n",
    "# Read docs\n",
    "all_docs, all_labels = read_documents('all_sentiment_shuffled.txt')\n",
    "# This split first 5% of data as training set, and the rest 20% will be evaluation set\n",
    "split_point = int(0.05*len(all_docs))\n",
    "train_docs = all_docs[:split_point]\n",
    "train_labels = all_labels[:split_point]\n",
    "eval_docs = all_docs[split_point:]\n",
    "eval_labels = all_labels[split_point:]\n",
    "\n",
    "# Train model get a dataframe which contains probabilities after smoothing and log\n",
    "df_probablities =train_nb(train_docs,train_labels)\n",
    "print(df_probablities)\n",
    "\n",
    "# Ref:https://docs.python.org/3.1/library/collections.html\n",
    "freqs = Counter()\n",
    "for label in train_labels:\n",
    "    freqs[label]+=1\n",
    "\n",
    "#Sum total labels\n",
    "#Ref: https://stackoverflow.com/questions/18593519/sum-of-all-counts-in-a-collections-counter\n",
    "print('Total labels:\\n',sum(freqs.values()))\n",
    "print('negtive labels:\\n',freqs['neg'])\n",
    "print('postive labels:\\n',freqs['pos'])\n",
    "neg_pob = calculate_probability(freqs['neg'],sum(freqs.values()))\n",
    "pos_pob = calculate_probability(freqs['pos'],sum(freqs.values()))\n",
    "print('negtive probablity after smoothing and log:\\n',neg_pob)\n",
    "print('postive probablity after smoothing and log:\\n',pos_pob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 1, 2, 1, 1, 'a'], [1, 2, 3, 1, 2, 1, 1, 'b'], [1, 2, 3, 1, 2, 1, 1, 'c']]\n"
     ]
    }
   ],
   "source": [
    "# Example for append function\n",
    "keys = ['a', 'b', 'c']\n",
    "values = [[1, 2, 3, 1, 2, 1, 1],[1, 2, 3, 1, 2, 1, 1],[1, 2, 3, 1, 2, 1, 1]]\n",
    "for index in range(len(values)):\n",
    "        values[index].append(keys[index])\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
